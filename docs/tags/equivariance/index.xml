<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Equivariance | Pavlo Melnyk</title>
    <link>/tags/equivariance/</link>
      <atom:link href="/tags/equivariance/index.xml" rel="self" type="application/rss+xml" />
    <description>Equivariance</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hu10934838655842282239.png</url>
      <title>Equivariance</title>
      <link>/tags/equivariance/</link>
    </image>
    
    <item>
      <title>On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation</title>
      <link>/publication/ontheroleofrotequiformhpe/</link>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>/publication/ontheroleofrotequiformhpe/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Equivariant Modelling for Catalysis on 2D MXenes</title>
      <link>/publication/equimodelmxene/</link>
      <pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate>
      <guid>/publication/equimodelmxene/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; Merging advanced computations with machine learning, we aim to accelerate the exploration of catalytic behaviour in novel materials. We focus on two-dimensional (2D) Ti$_2$CT$_y$ MXenes, whose versatile surface chemistry makes them particularly compelling candidates for catalysis. However, resolving their composition and structure under realistic conditions requires going beyond the systems typically studied with density functional theory (DFT), as the computational cost of such calculations limits accessible system sizes and timescales, calling instead for more efficient approaches. To address this challenge, we generate a comprehensive dataset of 50,000 DFT calculations for training and 10,000 for testing, encompassing both Ti$_2$CT$_y$ MXene configurations and molecular systems, along with an augmented dataset where systems are artificially repeated to investigate how well models generalise to larger systems.Employing advances in geometric deep learning, we train and validate an equivariant (\ie symmetry-aware) model (EquiformerV2) that accurately predicts atomic forces and formation energies &amp;mdash; quantities that DFT must repeatedly compute for structural and catalytic investigations &amp;mdash; for these 2D materials. This combined DFT–ML framework achieves computational acceleration of the order ${\sim}10^3$&amp;ndash;$10^4$ (on a CPU) while maintaining DFT-level accuracy (${\sim} {\pm} 45$ meV/Å for forces and ${\sim} {\pm} 6$ meV for per-atom energies), paving the way for more efficient investigations of MXene catalytic behaviour. Moreover, we confirm that the total energy prediction error of the model grows linearly with the number of atoms in an input system, while the force error remains the same, which, along with the equivariant model design, is a necessity for a robust model. The dataset and the trained models with the code are available at \url{https://github.com/CataLiUst}.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spherical NeurO($n$)s for Geometric Deep Learning</title>
      <link>/publication/thesis/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <guid>/publication/thesis/</guid>
      <description>&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;thesis_cover.png&#34; alt=&#34;Thesis Cover&#34; style=&#34;max-width:50%; height:auto;&#34; /&gt;
&lt;/p&gt; --&gt;
&lt;style&gt;
.page-header .featured-image img,
/* .article-container .featured-image img, */
div[class*=&#34;featured&#34;] img {
  max-width: 50% !important;
  width: 50% !important;
  height: auto !important;
  display: block !important;
  margin: 0 auto !important;
}
&lt;/style&gt;
&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; Felix Klein’s Erlangen Programme of 1872 introduced a methodology to unify non-Euclidean geometries. Similarly, geometric deep learning (GDL) constitutes a unifying framework for various neural network architectures. GDL is built from the first principles of geometry—symmetry and scale separation—and enables tractable learning in high dimensions. Symmetries play a vital role in preserving structural information of geometric data and allow models (i.e., neural networks) to adjust to different geometric transformations.\n\nIn this context, spheres exhibit a maximal set of symmetries compared to other geometric entities in Euclidean space. The orthogonal group O(n) fully encapsulates the symmetry structure of an nD sphere, including both rotational and reflection symmetries. In this thesis, we focus on integrating these symmetries into a model as an inductive bias, which is a crucial requirement for addressing problems in 3D vision as well as in natural sciences and their related applications.\n\nIn Paper A, we focus on 3D geometry and use the symmetries of spheres as geometric entities to construct neurons with spherical decision surfaces—spherical neurons—using a conformal embedding of Euclidean space. We also demonstrate that spherical neuron activations are non-linear due to the inherent non-linearity of the input embedding, and thus, do not necessarily require an activation function. In addition, we show graphically, theoretically, and experimentally that spherical neuron activations are isometries in Euclidean space, which is a prerequisite for the equivariance contributions of our subsequent work.\n\nIn Paper B, we closely examine the isometry property of the spherical neurons in the context of equivariance under 3D rotations (i.e., SO(3)-equivariance). Focusing on 3D in this work and based on a minimal set of four spherical neurons (one learned spherical decision surface and three copies), the centers of which are rotated into the corresponding vertices of a regular tetrahedron, we construct a spherical filter bank. We call it a steerable 3D spherical neuron because, as we verify later, it constitutes a steerable filter. Finally, we derive a 3D steerability constraint for a spherical neuron (i.e., a single spherical decision surface).\n\nIn Paper C, we present a learnable point-cloud descriptor invariant under 3D rotations and reflections, i.e., the O(3) actions, utilizing the steerable 3D spherical neurons we introduced previously, as well as vector neurons from related work. Specifically, we propose an embedding of the 3D steerable neurons into 4D vector neurons, which leverages end-to-end training of the model. The resulting model, termed TetraSphere, sets a new state-of-the-art performance classifying randomly rotated real-world object scans. Thus, our results reveal the practical value of steerable 3D spherical neurons for learning in 3D Euclidean space.\n\nIn Paper D, we generalize to nD the concepts we previously established in 3D, and propose O(n)-equivariant neurons with spherical decision surfaces, which we call Deep Equivariant Hyper-spheres. We demonstrate how to combine them in a network that directly operates on the basis of the input points and propose an invariant operator based on the relation between two points and a sphere, which as we show, turns out to be a Gram matrix.\n\nIn summary, this thesis introduces techniques based on spherical neurons that enhance the GDL framework, with a specific focus on equivariant and invariant learning on point sets. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>O$n$ Learning Deep O$(n)$ Equivariant Hyperspheres</title>
      <link>/publication/equihypers/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      <guid>/publication/equihypers/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; In this paper, we utilize hyperspheres and regular $n$-simplexes and propose an approach to learning deep features equivariant under the transformations of $n$D reflections and rotations, encompassed by the powerful group of $\text{O}(n)$. Namely, we propose $\text{O}(n)$-equivariant neurons with spherical decision surfaces that generalize to any dimension $n$, which we call &lt;em&gt;Deep Equivariant Hyperspheres&lt;/em&gt;. We demonstrate how to combine them in a network that directly operates on the basis of the input points and propose an invariant operator based on the relation between two points and a sphere, which as we show, turns out to be a Gram matrix. Using synthetic and real-world data in $n$D, we experimentally verify our theoretical contributions and find that our approach is superior to the competing methods for $\text{O}(n)$-equivariant benchmark datasets (classification and regression), demonstrating a favorable speed/performance trade-off. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis</title>
      <link>/publication/tetrasphere/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>/publication/tetrasphere/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; In many practical applications 3D point cloud analysis requires rotation invariance. In this paper we present a learnable descriptor invariant under 3D rotations and reflections i.e. the O(3) actions utilizing the recently introduced steerable 3D spherical neurons and vector neurons. Specifically we propose an embedding of the 3D spherical neurons into 4D vector neurons which leverages end-to-end training of the model. In our approach we perform TetraTransform&amp;mdash;an equivariant embedding of the 3D input into 4D constructed from the steerable neurons&amp;mdash;and extract deeper O(3)-equivariant features using vector neurons. This integration of the TetraTransform into the VN-DGCNN framework termed TetraSphere negligibly increases the number of parameters by less than 0.0002%. TetraSphere sets a new state-of-the-art performance classifying randomly rotated real-world object scans of the challenging subsets of ScanObjectNN. Additionally TetraSphere outperforms all equivariant methods on randomly rotated synthetic data: classifying objects from ModelNet40 and segmenting parts of the ShapeNet shapes. Thus our results reveal the practical value of steerable 3D spherical neurons for learning in 3D Euclidean space. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Steerable 3D Spherical Neurons</title>
      <link>/publication/steerneur/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      <guid>/publication/steerneur/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; Emerging from low-level vision theory, steerable filters found their counterpart in prior work on steerable convolutional neural networks equivariant to rigid transformations. In our work, we propose a steerable feed-forward learning-based approach that consists of neurons with spherical decision surfaces and operates on point clouds. Such spherical neurons are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Focusing on 3D geometry, we exploit the isometry property of spherical neurons and derive a 3D steerability constraint. After training spherical neurons to classify point clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate the neurons and construct rotation-equivariant spherical filter banks. We then apply the derived constraint to interpolate the filter bank outputs and, thus, obtain a rotation-invariant network. Finally, we use a synthetic point set and real-world 3D skeleton data to verify our theoretical findings. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
