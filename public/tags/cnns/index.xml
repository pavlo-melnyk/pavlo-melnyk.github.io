<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CNNs | Pavlo Melnyk</title>
    <link>//localhost:1313/tags/cnns/</link>
      <atom:link href="//localhost:1313/tags/cnns/index.xml" rel="self" type="application/rss+xml" />
    <description>CNNs</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 30 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:1313/media/icon_hu10934838655842282239.png</url>
      <title>CNNs</title>
      <link>//localhost:1313/tags/cnns/</link>
    </image>
    
    <item>
      <title>A High-Performance CNN Method for Offline Handwritten Chinese Character Recognition and Visualization</title>
      <link>//localhost:1313/publication/ahighperf/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/publication/ahighperf/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;display:none;&#34;&gt; Recent researches introduced fast, compact and efficient convolutional neural networks (CNNs) for offline handwritten Chinese character recognition (HCCR). However, many of them did not address the problem of network interpretability. We propose a new architecture of a deep CNN with high recognition performance which is capable of learning deep features for visualization. A special characteristic of our model is the bottleneck layers which enable us to retain its expressiveness while reducing the number of multiply-accumulate operations and the required storage. We introduce a modification of global weighted average pooling (GWAP) - global weighted output average pooling (GWOAP). This paper demonstrates how they allow us to calculate class activation maps (CAMs) in order to indicate the most relevant input character image regions used by our CNN to identify a certain class. Evaluating on the ICDAR-2013 offline HCCR competition dataset, we show that our model enables a relative 0.83% error reduction while having 49% fewer parameters and the same computational cost compared to the current state-of-the-art single-network method trained only on handwritten data. Our solution outperforms even recent residual learning approaches. &lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
