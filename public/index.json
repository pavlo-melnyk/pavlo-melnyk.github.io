[
      {
        "title": "CV",
        "permalink": "//localhost:1313/cv/",
        "summary": "\u003cp\u003eYou can view my CV \u003ca href=\"/uploads/cv.pdf\" target=\"_blank\" class=\"btn btn-primary\"\u003ehere\u003c/a\u003e.\u003c/p\u003e",
        "content": "You can view my CV here.\n"
      },
      {
        "title": "PhD Thesis",
        "permalink": "//localhost:1313/phd_thesis/",
        "summary": "\u003cp\u003eExplore my PhD thesis, titled \n, which I defended with \n serving as an opponent.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://liu.diva-portal.org/smash/get/diva2:1894492/FULLTEXT02.pdf\" target=\"_blank\"\u003e\n\u003cimg src=\"https://liu.diva-portal.org/smash/get/diva2:1894492/PREVIEW01.png)\" alt=\"Thesis Cover\" style=\"max-width:50%; height:auto;\" /\u003e\n  \u003c/a\u003e\n\u003c/p\u003e",
        "content": "Explore my PhD thesis, titled , which I defended with serving as an opponent.\n"
      },
      {
        "title": "Projects",
        "permalink": "//localhost:1313/projects/",
        "summary": "\u003cp\u003eFeel free to explore my \n, which includes projects from both my Master’s and PhD research.\u003c/p\u003e",
        "content": "Feel free to explore my , which includes projects from both my Master’s and PhD research.\n"
      },
      {
        "title": "Publications",
        "permalink": "//localhost:1313/publications/",
        "summary": "\u003cp\u003eYou can view all my publications on \n.\nA selection is presented \n.\u003c/p\u003e",
        "content": "You can view all my publications on . A selection is presented .\n"
      },
      {
        "title": "My PhD Defense",
        "permalink": "//localhost:1313/post/phd-defense/",
        "summary": "On this day, I defended my PhD thesis on \u0026ldquo;Spherical Neur$\\text{O}(n)$s for Geometric Deep Learning\u0026rdquo;.",
        "content": " I successfully defended my at Linköping University on September 27, 2024. It was an honor to have from the University of Pennsylvania serve as my opponent during the defense. With this milestone, I\u0026rsquo;ve earned my doctorate in Electrical Engineering, with a specialization in Computer Vision.\n"
      },
      {
        "title": "O$n$ Learning Deep O$(n)$ Equivariant Hyperspheres",
        "permalink": "//localhost:1313/publication/equihypers/",
        "summary": "\u003cp\u003e\u003cspan style=\"display:none;\"\u003e In this paper, we utilize hyperspheres and regular $n$-simplexes and propose an approach to learning deep features equivariant under the transformations of $n$D reflections and rotations, encompassed by the powerful group of $\\text{O}(n)$. Namely, we propose $\\text{O}(n)$-equivariant neurons with spherical decision surfaces that generalize to any dimension $n$, which we call \u003cem\u003eDeep Equivariant Hyperspheres\u003c/em\u003e. We demonstrate how to combine them in a network that directly operates on the basis of the input points and propose an invariant operator based on the relation between two points and a sphere, which as we show, turns out to be a Gram matrix. Using synthetic and real-world data in $n$D, we experimentally verify our theoretical contributions and find that our approach is superior to the competing methods for $\\text{O}(n)$-equivariant benchmark datasets (classification and regression), demonstrating a favorable speed/performance trade-off. \u003c/span\u003e\u003c/p\u003e",
        "content": " In this paper, we utilize hyperspheres and regular $n$-simplexes and propose an approach to learning deep features equivariant under the transformations of $n$D reflections and rotations, encompassed by the powerful group of $\\text{O}(n)$. Namely, we propose $\\text{O}(n)$-equivariant neurons with spherical decision surfaces that generalize to any dimension $n$, which we call Deep Equivariant Hyperspheres. We demonstrate how to combine them in a network that directly operates on the basis of the input points and propose an invariant operator based on the relation between two points and a sphere, which as we show, turns out to be a Gram matrix. Using synthetic and real-world data in $n$D, we experimentally verify our theoretical contributions and find that our approach is superior to the competing methods for $\\text{O}(n)$-equivariant benchmark datasets (classification and regression), demonstrating a favorable speed/performance trade-off. "
      },
      {
        "title": "TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis",
        "permalink": "//localhost:1313/publication/tetrasphere/",
        "summary": "\u003cp\u003e\u003cspan style=\"display:none;\"\u003e In many practical applications 3D point cloud analysis requires rotation invariance. In this paper we present a learnable descriptor invariant under 3D rotations and reflections i.e. the O(3) actions utilizing the recently introduced steerable 3D spherical neurons and vector neurons. Specifically we propose an embedding of the 3D spherical neurons into 4D vector neurons which leverages end-to-end training of the model. In our approach we perform TetraTransform\u0026mdash;an equivariant embedding of the 3D input into 4D constructed from the steerable neurons\u0026mdash;and extract deeper O(3)-equivariant features using vector neurons. This integration of the TetraTransform into the VN-DGCNN framework termed TetraSphere negligibly increases the number of parameters by less than 0.0002%. TetraSphere sets a new state-of-the-art performance classifying randomly rotated real-world object scans of the challenging subsets of ScanObjectNN. Additionally TetraSphere outperforms all equivariant methods on randomly rotated synthetic data: classifying objects from ModelNet40 and segmenting parts of the ShapeNet shapes. Thus our results reveal the practical value of steerable 3D spherical neurons for learning in 3D Euclidean space. \u003c/span\u003e\u003c/p\u003e",
        "content": " In many practical applications 3D point cloud analysis requires rotation invariance. In this paper we present a learnable descriptor invariant under 3D rotations and reflections i.e. the O(3) actions utilizing the recently introduced steerable 3D spherical neurons and vector neurons. Specifically we propose an embedding of the 3D spherical neurons into 4D vector neurons which leverages end-to-end training of the model. In our approach we perform TetraTransform\u0026mdash;an equivariant embedding of the 3D input into 4D constructed from the steerable neurons\u0026mdash;and extract deeper O(3)-equivariant features using vector neurons. This integration of the TetraTransform into the VN-DGCNN framework termed TetraSphere negligibly increases the number of parameters by less than 0.0002%. TetraSphere sets a new state-of-the-art performance classifying randomly rotated real-world object scans of the challenging subsets of ScanObjectNN. Additionally TetraSphere outperforms all equivariant methods on randomly rotated synthetic data: classifying objects from ModelNet40 and segmenting parts of the ShapeNet shapes. Thus our results reveal the practical value of steerable 3D spherical neurons for learning in 3D Euclidean space. "
      },
      {
        "title": "Steerable 3D Spherical Neurons",
        "permalink": "//localhost:1313/publication/steerneur/",
        "summary": "\u003cp\u003e\u003cspan style=\"display:none;\"\u003e Emerging from low-level vision theory, steerable filters found their counterpart in prior work on steerable convolutional neural networks equivariant to rigid transformations. In our work, we propose a steerable feed-forward learning-based approach that consists of neurons with spherical decision surfaces and operates on point clouds. Such spherical neurons are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Focusing on 3D geometry, we exploit the isometry property of spherical neurons and derive a 3D steerability constraint. After training spherical neurons to classify point clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate the neurons and construct rotation-equivariant spherical filter banks. We then apply the derived constraint to interpolate the filter bank outputs and, thus, obtain a rotation-invariant network. Finally, we use a synthetic point set and real-world 3D skeleton data to verify our theoretical findings. \u003c/span\u003e\u003c/p\u003e",
        "content": " Emerging from low-level vision theory, steerable filters found their counterpart in prior work on steerable convolutional neural networks equivariant to rigid transformations. In our work, we propose a steerable feed-forward learning-based approach that consists of neurons with spherical decision surfaces and operates on point clouds. Such spherical neurons are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Focusing on 3D geometry, we exploit the isometry property of spherical neurons and derive a 3D steerability constraint. After training spherical neurons to classify point clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate the neurons and construct rotation-equivariant spherical filter banks. We then apply the derived constraint to interpolate the filter bank outputs and, thus, obtain a rotation-invariant network. Finally, we use a synthetic point set and real-world 3D skeleton data to verify our theoretical findings. "
      },
      {
        "title": "Embed Me If You Can: A Geometric Perceptron",
        "permalink": "//localhost:1313/publication/embedme/",
        "summary": "\u003cp\u003e\u003cspan style=\"display:none;\"\u003e Solving geometric tasks involving point clouds by using machine learning is a challenging problem. Standard feed-forward neural networks combine linear or, if the bias parameter is included, affine layers and activation functions. Their geometric modeling is limited, which motivated the prior work introducing the multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the hypersphere neuron, is obtained by applying a conformal embedding of Euclidean space. By virtue of Clifford algebra, it can be implemented as the Cartesian dot product of inputs and weights. If the embedding is applied in a manner consistent with the dimensionality of the input space geometry, the decision surfaces of the model units become combinations of hyperspheres and make the decision-making process geometrically interpretable for humans. Our extension of the MLHP model, the multilayer geometric perceptron (MLGP), and its respective layer units, i.e., geometric neurons, are consistent with the 3D geometry and provide a geometric handle of the learned coefficients. In particular, the geometric neuron activations are isometric in 3D, which is necessary for rotation and translation equivariance. When classifying the 3D Tetris shapes, we quantitatively show that our model requires no activation function in the hidden layers other than the embedding to outperform the vanilla multilayer perceptron. In the presence of noise in the data, our model is also superior to the MLHP. \u003c/span\u003e\u003c/p\u003e",
        "content": " Solving geometric tasks involving point clouds by using machine learning is a challenging problem. Standard feed-forward neural networks combine linear or, if the bias parameter is included, affine layers and activation functions. Their geometric modeling is limited, which motivated the prior work introducing the multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the hypersphere neuron, is obtained by applying a conformal embedding of Euclidean space. By virtue of Clifford algebra, it can be implemented as the Cartesian dot product of inputs and weights. If the embedding is applied in a manner consistent with the dimensionality of the input space geometry, the decision surfaces of the model units become combinations of hyperspheres and make the decision-making process geometrically interpretable for humans. Our extension of the MLHP model, the multilayer geometric perceptron (MLGP), and its respective layer units, i.e., geometric neurons, are consistent with the 3D geometry and provide a geometric handle of the learned coefficients. In particular, the geometric neuron activations are isometric in 3D, which is necessary for rotation and translation equivariance. When classifying the 3D Tetris shapes, we quantitatively show that our model requires no activation function in the hidden layers other than the embedding to outperform the vanilla multilayer perceptron. In the presence of noise in the data, our model is also superior to the MLHP. "
      },
      {
        "title": "A High-Performance CNN Method for Offline Handwritten Chinese Character Recognition and Visualization",
        "permalink": "//localhost:1313/publication/ahighperf/",
        "summary": "\u003cp\u003e\u003cspan style=\"display:none;\"\u003e Recent researches introduced fast, compact and efficient convolutional neural networks (CNNs) for offline handwritten Chinese character recognition (HCCR). However, many of them did not address the problem of network interpretability. We propose a new architecture of a deep CNN with high recognition performance which is capable of learning deep features for visualization. A special characteristic of our model is the bottleneck layers which enable us to retain its expressiveness while reducing the number of multiply-accumulate operations and the required storage. We introduce a modification of global weighted average pooling (GWAP) - global weighted output average pooling (GWOAP). This paper demonstrates how they allow us to calculate class activation maps (CAMs) in order to indicate the most relevant input character image regions used by our CNN to identify a certain class. Evaluating on the ICDAR-2013 offline HCCR competition dataset, we show that our model enables a relative 0.83% error reduction while having 49% fewer parameters and the same computational cost compared to the current state-of-the-art single-network method trained only on handwritten data. Our solution outperforms even recent residual learning approaches. \u003c/span\u003e\u003c/p\u003e",
        "content": " Recent researches introduced fast, compact and efficient convolutional neural networks (CNNs) for offline handwritten Chinese character recognition (HCCR). However, many of them did not address the problem of network interpretability. We propose a new architecture of a deep CNN with high recognition performance which is capable of learning deep features for visualization. A special characteristic of our model is the bottleneck layers which enable us to retain its expressiveness while reducing the number of multiply-accumulate operations and the required storage. We introduce a modification of global weighted average pooling (GWAP) - global weighted output average pooling (GWOAP). This paper demonstrates how they allow us to calculate class activation maps (CAMs) in order to indicate the most relevant input character image regions used by our CNN to identify a certain class. Evaluating on the ICDAR-2013 offline HCCR competition dataset, we show that our model enables a relative 0.83% error reduction while having 49% fewer parameters and the same computational cost compared to the current state-of-the-art single-network method trained only on handwritten data. Our solution outperforms even recent residual learning approaches. "
      }
  ]